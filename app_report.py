import os
import requests
import time
from src import util
from src import config
import re
from datetime import datetime
from bs4 import BeautifulSoup

print(f'ì•Œë¦¼ì¥ ì‹œì‘')

# ì•Œë¦¼ì¥ ë§ˆì§€ë§‰ í˜ì´ì§€ êµ¬í•˜ê¸°
res = requests.get(config.REPORT_URL, headers=config.CUSTOM_HEADERS)
soup = BeautifulSoup(res.content, 'html.parser')
page_list = soup.find_all('a', class_='page-link')

number = 0
last_page = 0
for i in page_list:
    number = number + 1
last_page = page_list[number - 2].text

page = 0
while page <= int(last_page):
    page = page + 1

    # ì•Œë¦¼ì¥ í˜ì´ì§€ ë³„ ëª©ë¡
    report_page_url = config.REPORT_URL + '?page=' + str(page)
    res = requests.get(report_page_url, headers=config.CUSTOM_HEADERS)

    soup = BeautifulSoup(res.content, 'html.parser')
    a_list = soup.find('div', class_='report-list-wrapper').find_all('a')

    href_list = []
    for href in a_list:
        href_list.append(href['href'])

    for href_item in href_list:
        # ì•Œë¦¼ì¥ ìƒì„¸ í˜ì´ì§€
        detail_url = config.BASE_URL + href_item
        res = requests.get(detail_url, headers=config.CUSTOM_HEADERS)
        soup = BeautifulSoup(res.content, 'html.parser')

        # íƒ€ì´í‹€(ë‚ ì§œ ì¶”ì¶œ)
        sTitle = soup.find('h3', class_='sub-header-title').text.strip()
        sTitle_list = re.findall(r'\d+', sTitle)
        sTitle = sTitle_list[0] + \
            sTitle_list[1].zfill(2) + sTitle_list[2].zfill(2)
        dtTitle = datetime.strptime(sTitle, '%Y%m%d')
        title = datetime.strftime(dtTitle, '%Y%m%d')

        # íƒ€ì´í‹€ ëª…ìœ¼ë¡œ í´ë” ìƒì„±
        path = config.OUTPUT_ROOT + 'reports/' + title + '/'
        util.createFolder(path)

        # ë³¸ë¬¸ ì´ì˜ê²Œ
        contentString = soup.find('div', class_='content-text')
        contentString_list = []
        for s in contentString:
            contentString_list.append(s.text)
        content_body = '\n'.join(contentString_list)

        # ëŒ“ê¸€
        comment_list = soup.find(
            'ul', class_='comment-list').find_all('li', class_='comment')
        if comment_list is not None:
            comment_body_list = []
            for comment_item in comment_list:
                comment_author = comment_item.find(
                    'span', class_='author-name').text
                comment_time = comment_item.find(
                    'span', class_='date-written').text
                commentString = comment_item.find('p')
                commentString_list = []
                for s in commentString:
                    commentString_list.append(s.text)
                commentStr = '\n'.join(commentString_list)
                comment_body_list.append('â†ªï¸' + comment_author + '\n' +
                                         comment_time + '\n' + commentStr + '\n')
            comment_body = '\n'.join(comment_body_list)

        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë³¸ë¬¸ ëŒ“ê¸€ ì €ì¥
        filename = path + title + '.txt'
        report = 'ğŸ•§ ì¼ì‹œ' + '\n' + title + '\n\n' + \
            'ğŸŸ  ë³¸ë¬¸' + '\n' + content_body + '\n\n' + \
            'ğŸŸ  ëŒ“ê¸€' + '\n' + comment_body

        print(f'{filename} ìƒì„±')
        util.SaveFile(filename, report)

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        img_grid = soup.find('div', class_='image-section')
        if img_grid is not None:
            imgs = img_grid.find_all('div', class_='grid')
            for img in imgs:
                download_url = img.find('a')['data-download']
                name, ext = os.path.splitext(download_url)
                img_name = title + '-' + img['data-index'] + ext
                start = time.time()

                fullPath = path + img_name
                res = requests.get(download_url)

                print(f'{fullPath} ìƒì„±')
                os.system(f'curl "{download_url}" --output {fullPath}')

print(f'ì¢…ë£Œ')
