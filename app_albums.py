import os
import requests
import time
from src import util
from src import config
import re
from datetime import datetime
from bs4 import BeautifulSoup
import imgkit
from html2image import Html2Image

print(f'ì•¨ë²” ì‹œì‘')

# ì•¨ë²” ë§ˆì§€ë§‰ í˜ì´ì§€ êµ¬í•˜ê¸°
res = requests.get(config.ALNUMS_URL, headers=config.CUSTOM_HEADERS)
soup = BeautifulSoup(res.content, 'html.parser')

page_list = soup.find_all('a', class_='page-link')

number = 0
last_page = 0
for i in page_list:
    number = number + 1
last_page = page_list[number - 2].text

page = 0
while page <= int(last_page):
    page = page + 1

    # ì•¨ë²” í˜ì´ì§€ ë³„ ëª©ë¡
    albums_page_url = config.ALNUMS_URL + '?page=' + str(page)
    res = requests.get(albums_page_url, headers=config.CUSTOM_HEADERS)
    soup = BeautifulSoup(res.content, 'html.parser')
    a_list = soup.find('div', class_='album-list-wrapper').find_all('a')

    href_list = []
    for href in a_list:
        href_list.append(href['href'])

    for href_item in href_list:
        # ì•Œë¦¼ì¥ ìƒì„¸ í˜ì´ì§€
        detail_url = config.BASE_URL + href_item
        res = requests.get(detail_url, headers=config.CUSTOM_HEADERS)
        soup = BeautifulSoup(res.content, 'html.parser')

        # ë“±ë¡ì¼ ì¶”ì¶œ
        sCreated_at = soup.find('span', class_='date').text.strip()
        sCreated_at_list = re.findall(r'\d+', sCreated_at)
        sCreated_at = sCreated_at_list[0] + \
            sCreated_at_list[1].zfill(2) + sCreated_at_list[2].zfill(
                2) + sCreated_at_list[3].zfill(2) + sCreated_at_list[4].zfill(2)
        dtCreated_at = datetime.strptime(sCreated_at, '%Y%m%d%I%M')
        sCreated_at = datetime.strftime(dtCreated_at, '%Y%m%d%I%M')

        # íƒ€ì´í‹€(ì œëª© ì¶”ì¶œ)
        sTitle = soup.find('h3', class_='sub-header-title').text.strip()

        # íƒ€ì´í‹€ ëª…ìœ¼ë¡œ í´ë” ìƒì„±
        path = config.OUTPUT_ROOT + 'albums/' + sCreated_at + '/'
        util.createFolder(path)

        # ë³¸ë¬¸ ì´ì˜ê²Œ
        contentString = soup.find('div', class_='content-text')
        contentString_list = []
        for s in contentString:
            contentString_list.append(s.text)
        content_body = '\n'.join(contentString_list)

        # ëŒ“ê¸€
        comment_list = soup.find(
            'ul', class_='comment-list').find_all('li', class_='comment')
        if comment_list is not None:
            comment_body_list = []
            for comment_item in comment_list:
                comment_author = comment_item.find(
                    'span', class_='author-name').text
                comment_time = comment_item.find(
                    'span', class_='date-written').text
                commentString = comment_item.find('p')
                commentString_list = []
                for s in commentString:
                    commentString_list.append(s.text)
                commentStr = '<br/>'.join(commentString_list)
                comment_body_list.append('â†ªï¸' + comment_author + '<br/>' +
                                         comment_time + '<br/>' + commentStr + '<br/>')
            comment_body = '<br/>'.join(comment_body_list)

        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë³¸ë¬¸ ëŒ“ê¸€ ì €ì¥
        # filename = path + sCreated_at + '.txt'
        # report = 'ğŸ•§ ì¼ì‹œ' + '\n' + sCreated_at + '\n\n' + \
        #     'ğŸŸ  ì œëª©' + '\n' + sTitle + '\n\n' + \
        #     'ğŸŸ  ë³¸ë¬¸' + '\n' + content_body + '\n\n' + \
        #     'ğŸŸ  ëŒ“ê¸€' + '\n' + comment_body

        # print(f'{filename} ìƒì„±')
        # util.SaveFile(filename, report)

        # ìŠ¤í¬ë¦°ìƒ· íŒŒì¼ ìƒì„±
        htmlFile = f'{path}{sCreated_at}.html'
        ImgFile = f'{sCreated_at}_screenshot.jpg'
        print(f'{path}{ImgFile} í™”ë©´ ìƒì„±')
        # util.SaveFile(htmlFile, res.text)

        util.HtmlToImageWithSelenium(
            header_url=util.getCookiesFromDomain(
                'kidsnote', ''), url=detail_url, output_file='1.png')
        # hti = Html2Image(output_path=path)
        # hti.screenshot(html_file=htmlFile, size=[2000, 2000], save_as=ImgFile)

        exit()

        # ë™ì˜ìƒ ë‹¤ìš´ë¡œë“œ
        video_section = soup.find('div', class_='video-section')
        if video_section is not None:
            source = soup.find('source')['src']

            download_url = source
            name, ext = os.path.splitext(download_url)
            video_name = sCreated_at + ext
            fullPath = path + video_name

            print(f'{fullPath} ìƒì„±')
            os.system(f'curl "{download_url}" --output {fullPath}')

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        img_grid = soup.find('div', class_='image-section')
        if img_grid is not None:
            imgs = img_grid.find_all('div', class_='grid')
            for img in imgs:
                download_url = img.find('a')['data-download']
                name, ext = os.path.splitext(download_url)
                img_name = sCreated_at + '-' + img['data-index'] + ext

                fullPath = path + img_name
                res = requests.get(download_url)

                print(f'{fullPath} ìƒì„±')
                os.system(f'curl "{download_url}" --output {fullPath}')

print(f'ì¢…ë£Œ')
